{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "weirdtargets2020.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNv7jXu00cNVgGsmh8lr3eu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlyShmahell/WeirdTargets/blob/master/weirdtargets2020.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7-LqJsEsxIS"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZT_SJpk4fwB",
        "outputId": "77b80ebc-4795-434b-a899-c3668ef12584"
      },
      "source": [
        "! pip install ujson"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ujson in /usr/local/lib/python3.6/dist-packages (4.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvXS15KXsBCO"
      },
      "source": [
        "import os\n",
        "import gc\n",
        "import sys\n",
        "import ujson\n",
        "import psutil\n",
        "import requests\n",
        "import numpy     as     np\n",
        "import pandas    as     pd\n",
        "import operator  as     op\n",
        "from   copy      import copy\n",
        "from   tqdm      import tqdm\n",
        "from   functools import reduce\n",
        "from   gzip      import GzipFile\n",
        "from   toolz     import partition_all\n",
        "from   itertools import islice\n",
        "from   pathlib   import Path\n",
        "from   collections import Counter\n",
        "from   sklearn.cluster import KMeans\n",
        "from   concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPh4UBdkspkV"
      },
      "source": [
        "# nCr"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NST-Tgdsq3Y"
      },
      "source": [
        "def ncr(n, r):\n",
        "    r     = min(r, n-r)\n",
        "    numer = reduce(op.mul, range(n, n-r, -1), 1)\n",
        "    denom = reduce(op.mul, range(1, r+1), 1)\n",
        "    return  numer / denom"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqaXhyOFsuZV"
      },
      "source": [
        "# Downloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIT5D5YksvrY"
      },
      "source": [
        "def downloader(url):\n",
        "    def func(r, path, filename, total_size, block_size):\n",
        "        t          = tqdm(\n",
        "            desc       = f\"downloading {filename}\",\n",
        "            total      = total_size, \n",
        "            unit       = 'iB', \n",
        "            unit_scale = True\n",
        "        )\n",
        "        with open(path/filename, 'wb') as f:\n",
        "            for data in r.iter_content(block_size):\n",
        "                t.update(len(data))\n",
        "                f.write(data)\n",
        "        t.close()\n",
        "        try:\n",
        "            assert not(total_size != 0 and t.n != total_size)\n",
        "        except:\n",
        "            sys.exit(f\"downloaded {t.n}/{total_size}\", flush=True)\n",
        "    r          = requests.get(url, stream=True)\n",
        "    total_size = int(r.headers.get('content-length', 0))\n",
        "    block_size = 1024**2\n",
        "    filename   = url.split('/')[-1]\n",
        "    path       = Path(Path.home())/Path('workspace')\n",
        "    path.mkdir(parents=True, exist_ok=True)\n",
        "    try:\n",
        "        assert Path(path/filename).stat().st_size == total_size\n",
        "        print(f\"{filename} is already downloaded\", flush=True)\n",
        "    except:\n",
        "        func(r, path, filename, total_size, block_size)\n",
        "    filename    = filename.split('.')\n",
        "    (filename, \n",
        "     extension) = \".\".join(filename[:-1]), filename[-1]\n",
        "    return path, filename, extension"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hamNVFrssLte"
      },
      "source": [
        "class Prerocessor:\n",
        "    def __traverse__(self, parsed, keys):\n",
        "        if len(keys)>1:\n",
        "            return self.__traverse__(parsed[keys[0]], keys[1:])\n",
        "        return parsed[keys[0]]\n",
        "    def __json__(self, string):\n",
        "        parsed = ujson.loads(string)\n",
        "        obj = {\n",
        "            key: self.__traverse__(parsed, self.keygroup[key])\n",
        "            for key in self.keygroup.keys()\n",
        "        }\n",
        "        return obj\n",
        "    def __df__(self, batch):\n",
        "        json = map(lambda string: self.__json__(string), batch)\n",
        "        df   = pd.DataFrame.from_records(json, \n",
        "                                         columns=[\"target\",\n",
        "                                                  \"disease\",\n",
        "                                                  \"score\"]\n",
        "        )\n",
        "        return df\n",
        "    def __init__(self, url, keygroup):\n",
        "        (path, \n",
        "         filename, \n",
        "         extension)   = downloader(url)\n",
        "        self.keygroup = keygroup\n",
        "        \"\"\"slices = [0]\n",
        "        size    = 0\n",
        "        bs      = int(12.8e+7)\n",
        "        with tqdm(desc=f\"Feature Probing\") as tqdmo:\n",
        "            with GzipFile(f\"{path}/{filename}.{extension}\") as f:\n",
        "                for i, b in enumerate(f):\n",
        "                    if size > bs:\n",
        "                        slices += [i-1]\n",
        "                        size    = 0\n",
        "                    size += (33+len(b))\n",
        "                    tqdmo.update(1)\n",
        "                slices += [i]\n",
        "        slices = [\n",
        "                  jdx - idx\n",
        "                  for idx, jdx in zip(slices[:-1], slices[1:])\n",
        "        ]\n",
        "        n = max(slices)\"\"\"\n",
        "        n = 952457\n",
        "        with tqdm(desc=f\"Feature Extraction, n={n}\") as tqdmo:\n",
        "            with GzipFile(f\"{path}/{filename}.{extension}\") as f:\n",
        "                batches = partition_all(n, f)\n",
        "                for batch in batches:\n",
        "                    df    = self.__df__(batch)\n",
        "                    df.to_hdf(\n",
        "                        f\"{path}/{filename}.hdf\",\n",
        "                        key='features',\n",
        "                        mode='a',\n",
        "                        format='table',\n",
        "                        append=True\n",
        "                    )\n",
        "                    tqdmo.update(n)\n",
        "        self.df = pd.read_hdf(\n",
        "                f\"{path}/{filename}.hdf\",\n",
        "                key='features'\n",
        "            )\n",
        "        print(len(self.df.index))\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-Geaf4SFclQ",
        "outputId": "71cc72e5-fc1d-4ef6-af68-551cc15eff01"
      },
      "source": [
        "%timeit\n",
        "Prerocessor(\n",
        "    'https://storage.googleapis.com/open-targets-data-releases/17.12/17.12_evidence_data.json.gz',\n",
        "    {\n",
        "        \"target\" : [\"target\", \"id\"],\n",
        "        \"disease\": [\"disease\", \"id\"],\n",
        "        \"score\":   [\"scores\", \"association_score\"]\n",
        "    }\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "17.12_evidence_data.json.gz is already downloaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Feature Extraction, n=952457: 952457it [01:55, 8273.33it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGSaSEcrhICE"
      },
      "source": [
        "!ls -al /root/workspace"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJBEf5Xll_vI"
      },
      "source": [
        "\r\n",
        "!rm -rf /root/workspace/*.hdf\r\n",
        "!ls /root/workspace"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pjr4kc_hr-QD"
      },
      "source": [
        "S = []\r\n",
        "with tqdm(desc=f\"Feature Probing\") as tqdmo:\r\n",
        "     with GzipFile(f\"/root/workspace/17.12_evidence_data.json.gz\") as f:\r\n",
        "        for b in f:\r\n",
        "            S += [len(b)+33]\r\n",
        "            tqdmo.update(1)\r\n",
        "with open('lens.json', 'w') as f:\r\n",
        "    ujson.dump(S, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYZ_neLcrunv",
        "outputId": "795f8c9f-a757-42c1-9aa1-4b82e0613693"
      },
      "source": [
        "with open('lens.json', 'r') as f:\r\n",
        "    S = np.array(ujson.load(f))\r\n",
        "    print(S)\r\n",
        "\r\n",
        "\r\n",
        "def max_subarray(S):\r\n",
        "    numbers = S.copy()\r\n",
        "    mean = np.mean(numbers)\r\n",
        "    numbers = numbers - mean\r\n",
        "    \"\"\"Find a contiguous subarray with the largest sum.\"\"\"\r\n",
        "    best_sum = 0  # or: float('-inf')\r\n",
        "    best_start = best_end = 0  # or: None\r\n",
        "    current_sum = 0\r\n",
        "    for current_end, x in enumerate(numbers):\r\n",
        "        if current_sum <= 0:\r\n",
        "            # Start a new sequence at the current element\r\n",
        "            current_start = current_end\r\n",
        "            current_sum = x\r\n",
        "        else:\r\n",
        "            # Extend the existing sequence with the current element\r\n",
        "            current_sum += x\r\n",
        "\r\n",
        "        if current_sum > best_sum and current_sum+((current_end-current_start)*mean) < int(6040779371/2):\r\n",
        "            best_sum = current_sum\r\n",
        "            best_start = current_start\r\n",
        "            best_end = current_end + 1  # the +1 is to make 'best_end' exclusive\r\n",
        "\r\n",
        "    return best_sum, best_start, best_end\r\n",
        "\r\n",
        "_, a, b = max_subarray(S)\r\n",
        "print(sum(S[a:b]), b-a)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2525 2440 2623 ... 6565 5437 6160]\n",
            "3020393697 475395\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}