{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "weirdtargets2020.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNdE+C8E3O6QqZE9HsWTMaj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlyShmahell/WeirdTargets/blob/master/weirdtargets2020.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7-LqJsEsxIS"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZT_SJpk4fwB",
        "outputId": "724f4eef-3db4-4fe9-8886-a28142b1b92e"
      },
      "source": [
        "! pip install ujson"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ujson in /usr/local/lib/python3.6/dist-packages (4.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvXS15KXsBCO"
      },
      "source": [
        "import os\n",
        "import gc\n",
        "import sys\n",
        "import ujson\n",
        "import psutil\n",
        "import requests\n",
        "import itertools\n",
        "import numpy              as     np\n",
        "import pandas             as     pd\n",
        "import operator           as     op\n",
        "from   copy               import copy\n",
        "from   tqdm               import tqdm\n",
        "from   functools          import reduce\n",
        "from   gzip               import GzipFile\n",
        "from   toolz              import partition_all\n",
        "from   pathlib            import Path\n",
        "from   concurrent.futures import ProcessPoolExecutor"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPh4UBdkspkV"
      },
      "source": [
        "# nCr"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NST-Tgdsq3Y"
      },
      "source": [
        "def ncr(n, r):\n",
        "    r     = min(r, n-r)\n",
        "    numer = reduce(op.mul, range(n, n-r, -1), 1)\n",
        "    denom = reduce(op.mul, range(1, r+1), 1)\n",
        "    return  numer / denom"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5vVDNyiB6WP"
      },
      "source": [
        "# msuba"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QRIe0gJB1T5"
      },
      "source": [
        "def msuba(S, t_size):\r\n",
        "    numbers     = S.copy()\r\n",
        "    mean        = np.mean(numbers)\r\n",
        "    std         = np.std(numbers)\r\n",
        "    numbers     = (numbers - mean)/std\r\n",
        "    best_sum    = 0\r\n",
        "    best_start  = best_end = 0\r\n",
        "    current_sum = 0\r\n",
        "    for current_end, x in enumerate(numbers):\r\n",
        "        if current_sum <= 0:\r\n",
        "            current_start = current_end\r\n",
        "            current_sum   = x\r\n",
        "        else:\r\n",
        "            current_sum  += x\r\n",
        "        if current_sum > best_sum and current_sum*std+((current_end-current_start)*mean) < t_size:\r\n",
        "            best_sum   = current_sum\r\n",
        "            best_start = current_start\r\n",
        "            best_end   = current_end + 1\r\n",
        "    return best_end - best_start"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqaXhyOFsuZV"
      },
      "source": [
        "# Downloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIT5D5YksvrY"
      },
      "source": [
        "def downloader(url):\n",
        "    def func(r, path, filename, total_size, block_size):\n",
        "        t          = tqdm(\n",
        "            desc       = f\"downloading {filename}\",\n",
        "            total      = total_size, \n",
        "            unit       = 'iB', \n",
        "            unit_scale = True\n",
        "        )\n",
        "        with open(path/filename, 'wb') as f:\n",
        "            for data in r.iter_content(block_size):\n",
        "                t.update(len(data))\n",
        "                f.write(data)\n",
        "        t.close()\n",
        "        try:\n",
        "            assert not(total_size != 0 and t.n != total_size)\n",
        "        except:\n",
        "            sys.exit(f\"downloaded {t.n}/{total_size}\", flush=True)\n",
        "    r          = requests.get(url, stream=True)\n",
        "    total_size = int(r.headers.get('content-length', 0))\n",
        "    block_size = 1024**2\n",
        "    filename   = url.split('/')[-1]\n",
        "    path       = Path(Path.home())/Path('workspace')\n",
        "    path.mkdir(parents=True, exist_ok=True)\n",
        "    try:\n",
        "        assert Path(path/filename).stat().st_size == total_size\n",
        "        print(f\"{filename} is already downloaded\", flush=True)\n",
        "    except:\n",
        "        func(r, path, filename, total_size, block_size)\n",
        "    filename    = filename.split('.')\n",
        "    (filename, \n",
        "     extension) = \".\".join(filename[:-1]), filename[-1]\n",
        "    return path, filename, extension"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtkdvHHRB9Kz"
      },
      "source": [
        "# Preprocessor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hamNVFrssLte"
      },
      "source": [
        "class Prerocessor:\n",
        "    def __traverse__(self, parsed, keys):\n",
        "        if len(keys)>1:\n",
        "            return self.__traverse__(parsed[keys[0]], keys[1:])\n",
        "        return parsed[keys[0]]\n",
        "    def __json__(self, string):\n",
        "        parsed = ujson.loads(string)\n",
        "        obj = {\n",
        "            key: self.__traverse__(parsed, self.keygroup[key])\n",
        "            for key in self.keygroup.keys()\n",
        "        }\n",
        "        return obj\n",
        "    def __df__(self, batch):\n",
        "        json = map(self.__json__, batch)\n",
        "        df   = pd.DataFrame.from_records(json, \n",
        "                                         columns=[\"target\",\n",
        "                                                  \"disease\",\n",
        "                                                  \"score\"]\n",
        "        )\n",
        "        return df\n",
        "    def __init__(self, url, keygroup):\n",
        "        (path, \n",
        "         filename, \n",
        "         extension)   = downloader(url)\n",
        "        self.keygroup = keygroup\n",
        "        S = []\n",
        "        with tqdm(desc=f\"Feature Probing\") as tqdmo:\n",
        "            with GzipFile(f\"{path}/{filename}.{extension}\") as f:\n",
        "                for b in f:\n",
        "                    S += [33+len(b)]\n",
        "                    tqdmo.update(1)\n",
        "        n = msuba(S, int(psutil.virtual_memory().available/5))\n",
        "        with tqdm(desc=f\"Feature Extraction, n={n}\") as tqdmo:\n",
        "            with GzipFile(f\"{path}/{filename}.{extension}\") as f:\n",
        "                batches = partition_all(n, f)\n",
        "                for batch in batches:\n",
        "                    df    = self.__df__(batch)\n",
        "                    df.to_hdf(\n",
        "                        f\"{path}/{filename}.hdf\",\n",
        "                        key='features',\n",
        "                        mode='a',\n",
        "                        format='table',\n",
        "                        append=True\n",
        "                    )\n",
        "                    tqdmo.update(len(df.index))\n",
        "        self.df = pd.read_hdf(\n",
        "                f\"{path}/{filename}.hdf\",\n",
        "                key='features'\n",
        "            )\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-Geaf4SFclQ",
        "outputId": "19de92d7-55a6-4df5-8976-89c330d01b8d"
      },
      "source": [
        "%timeit\n",
        "data = Prerocessor(\n",
        "    'https://storage.googleapis.com/open-targets-data-releases/17.12/17.12_evidence_data.json.gz',\n",
        "    {\n",
        "        \"target\" : [\"target\", \"id\"],\n",
        "        \"disease\": [\"disease\", \"id\"],\n",
        "        \"score\":   [\"scores\", \"association_score\"]\n",
        "    }\n",
        ")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "17.12_evidence_data.json.gz is already downloaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Feature Probing: 5784597it [03:10, 30314.68it/s]\n",
            "Feature Extraction, n=492474: 5909688it [07:29, 13140.03it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wokwldht8N3f"
      },
      "source": [
        "class CycleEnumeration:\r\n",
        "    def hash(self):\r\n",
        "        with tqdm(desc=f\"Hashing\") as tqdmo:\r\n",
        "            def setter(x):\r\n",
        "                tqdmo.update(1)\r\n",
        "                return set(x.to_numpy())\r\n",
        "            self.table = self.df.groupby('target')['disease'].apply(setter).to_numpy()\r\n",
        "    def map(self):\r\n",
        "        self.hash()\r\n",
        "        combinations = itertools.combinations(range(self.table.shape[0]), 2)\r\n",
        "        nck          = int(ncr(self.table.shape[0], 2))\r\n",
        "        base         = int(nck/os.cpu_count())\r\n",
        "        self.chunks  = []\r\n",
        "        cc           = 0\r\n",
        "        partition    = 0\r\n",
        "        with tqdm(desc=f\"Mapping\") as tqdmo:\r\n",
        "            for successive in range(base, nck, base):\r\n",
        "                self.chunks.append(\r\n",
        "                                {\r\n",
        "                                    \"num\": cc,\r\n",
        "                                    \"len\": base,\r\n",
        "                                    \"val\": itertools.islice(\r\n",
        "                                                combinations,\r\n",
        "                                                partition,\r\n",
        "                                                successive,\r\n",
        "                                                1\r\n",
        "                                            ) \r\n",
        "                                }\r\n",
        "                            )\r\n",
        "                partition  = successive\r\n",
        "                cc        += 1\r\n",
        "                tqdmo.update(base)\r\n",
        "            if successive < nck:\r\n",
        "                self.chunks.append(\r\n",
        "                                {\r\n",
        "                                    \"num\": cc,\r\n",
        "                                    \"len\": nck,\r\n",
        "                                    \"val\": itertools.islice(\r\n",
        "                                                combinations,\r\n",
        "                                                successive,\r\n",
        "                                                nck,\r\n",
        "                                                1\r\n",
        "                                            ) \r\n",
        "                                }\r\n",
        "                            )\r\n",
        "                tqdmo.update(base)\r\n",
        "    def worker(self, chunk):\r\n",
        "        local = 0\r\n",
        "        num = chunk[\"num\"]\r\n",
        "        with tqdm(desc=f'Reducing - pool #{num}', total=chunk['len'], leave=True, file=sys.stdout, position=0) as tqdmo:\r\n",
        "            for combination in chunk['val']:\r\n",
        "                common = self.table[combination[0]] & self.table[combination[1]]\r\n",
        "                if len(common) >= 2:\r\n",
        "                    local += 1\r\n",
        "                tqdmo.update(1)\r\n",
        "        return local\r\n",
        "    def reduce(self):\r\n",
        "        with ProcessPoolExecutor() as executor:\r\n",
        "            running_tasks = executor.map(self.worker, self.chunks)\r\n",
        "            for running_task in running_tasks:\r\n",
        "                self.counter += running_task\r\n",
        "    def __init__(self, df):\r\n",
        "        self.counter = 0\r\n",
        "        self.df = df\r\n",
        "        self.map()\r\n",
        "        self.reduce()\r\n",
        "        print(self.counter)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynBqeV_EDv9D",
        "outputId": "1c8689d3-48ea-492c-9e83-fd0135f7cb80"
      },
      "source": [
        "CycleEnumeration(data.df)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hashing: 33109it [00:14, 2350.36it/s] \n",
            "Mapping: 548086386it [00:00, 649023410825.90it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Reducing - pool #1:  50%|█████     | 274043193/548086386 [11:34<11:34, 394552.96it/s]\n",
            "Reducing - pool #0: 100%|██████████| 274043193/274043193 [17:17<00:00, 264181.29it/s]\n",
            "121114622\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.CycleEnumeration at 0x7f79d6060978>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    }
  ]
}